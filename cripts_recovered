==================== recovery/vscode_history/003i.py ====================
import os
import nibabel as nib
import numpy as np
import pandas as pd
from medpy.metric import binary
from tqdm import tqdm

print("Script started")

pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/gt_flat"
output_csv = "bcbm_fold3_val_dice.csv"
pred_files = sorted(f for f in os.listdir(pred_dir) if f.endswith(".nii.gz"))
print(f"Found {len(pred_files)} predicted files in: {pred_dir}")

results = []
for pred_file in tqdm(pred_files):
    pred_path = os.path.join(pred_dir, pred_file)
    gt_filename = pred_file.replace(".nii.gz", "_0000.nii.gz")
    gt_path = os.path.join(gt_dir, gt_filename)

    if not os.path.exists(gt_path):
        print(f" Missing ground truth: {gt_path}")
        continue

==================== recovery/vscode_history/0DdJ.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import warnings
import time

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
==================== recovery/vscode_history/12EK.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import time
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
    TF = None
    InterpolationMode = None
==================== recovery/vscode_history/1E67.py ====================
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nibabel as nib
from skimage import measure

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis"
os.makedirs(out_dir, exist_ok=True)

# === Load data and select top 3 patients ===
df = pd.read_csv(os.path.join(out_dir, "dice_with_hausdorff.csv"))
top3 = df.sort_values("Dice", ascending=False).head(3)

def plot_patient_contour(pid, dice, hd, out_path):
    try:
        img = nib.load(os.path.join(image_dir, f"{pid}_0000.nii.gz")).get_fdata()
        gt = nib.load(os.path.join(gt_dir, f"{pid}.nii.gz")).get_fdata()
        pred = nib.load(os.path.join(pred_dir, f"{pid}.nii.gz")).get_fdata()
==================== recovery/vscode_history/1Fcw.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import time
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
    TF = None
    InterpolationMode = None
==================== recovery/vscode_history/1Pls.py ====================
import inspect
import multiprocessing
import os
import shutil
import sys
import warnings
from copy import deepcopy
from datetime import datetime
from time import time, sleep
from typing import Tuple, Union, List

import numpy as np
import torch
from batchgenerators.dataloading.multi_threaded_augmenter import MultiThreadedAugmenter
from batchgenerators.dataloading.nondet_multi_threaded_augmenter import NonDetMultiThreadedAugmenter
from batchgenerators.dataloading.single_threaded_augmenter import SingleThreadedAugmenter
from batchgenerators.utilities.file_and_folder_operations import join, load_json, isfile, save_json, maybe_mkdir_p
from batchgeneratorsv2.helpers.scalar_type import RandomScalar
from batchgeneratorsv2.transforms.base.basic_transform import BasicTransform
from batchgeneratorsv2.transforms.intensity.brightness import MultiplicativeBrightnessTransform
from batchgeneratorsv2.transforms.intensity.contrast import ContrastTransform, BGContrast
from batchgeneratorsv2.transforms.intensity.gamma import GammaTransform
from batchgeneratorsv2.transforms.intensity.gaussian_noise import GaussianNoiseTransform
from batchgeneratorsv2.transforms.nnunet.random_binary_operator import ApplyRandomBinaryOperatorTransform
from batchgeneratorsv2.transforms.nnunet.remove_connected_components import \
==================== recovery/vscode_history/1y3E.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse, os
import torch, torch.nn as nn, torch.optim as optim
import numpy as np, pandas as pd, nibabel as nib
from torch.utils.data import Dataset, DataLoader

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        def block(ic,oc):
            return nn.Sequential(
                nn.Conv3d(ic, oc, 3, padding=1, bias=False),
                nn.BatchNorm3d(oc), nn.ReLU(inplace=True),
                nn.Conv3d(oc, oc, 3, padding=1, bias=False),
                nn.BatchNorm3d(oc), nn.ReLU(inplace=True),
            )
        self.enc1 = block(in_ch, base_feats)
        self.enc2 = block(base_feats, base_feats*2)
        self.enc3 = block(base_feats*2, base_feats*4)
        self.enc4 = block(base_feats*4, base_feats*8)
        self.bot  = block(base_feats*8, base_feats*16)
        self.up4, self.dec4 = nn.ConvTranspose3d(base_feats*16, base_feats*8,2,2), block(base_feats*16, base_feats*8)
        self.up3, self.dec3 = nn.ConvTranspose3d(base_feats*8,  base_feats*4,2,2), block(base_feats*8,  base_feats*4)
==================== recovery/vscode_history/2G2p.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd # Import pandas for CSV output
import seaborn as sns # Import seaborn for nicer plots

print("Starting FGSM attack script...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
# export nnUNet_results="/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# export nnUNet_compile=False # Disable torch.compile for checkpoint loading compatibility
==================== recovery/vscode_history/2tbG.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap

# --- Matplotlib Configuration for Dissertation Quality ---
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman']
plt.rcParams['mathtext.fontset'] = 'stix'

def find_best_slice(mask_data):
    """
    Finds the slice index with the largest segmentation area for each view.
    This ensures the visualization focuses on the most relevant part of the image.
    """
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[0] // 2, shape[1] // 2 # Z, X, Y for Axial, Sagittal, Coronal
    
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()    # Slice along Z
    sagittal_slice = np.sum(mask_data, axis=(1, 2)).argmax()  # Slice along X
    coronal_slice = np.sum(mask_data, axis=(0, 2)).argmax()   # Slice along Y
==================== recovery/vscode_history/3dOS.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd # Import pandas for CSV output
import seaborn as sns # Import seaborn for nicer plots

print("Starting FGSM attack script...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
# export nnUNet_results="/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# export nnUNet_compile=False # Disable torch.compile for checkpoint loading compatibility
==================== recovery/vscode_history/3ufo.py ====================
# Place these os.environ assignments AT THE VERY TOP OF YOUR SCRIPT
# before any other imports, especially nnunetv2 imports.
import os

# === Forcefully set nnUNet environment variables at the earliest point ===
# This ensures nnUNetTrainer can find its base paths regardless of shell exports
# or nnunetv2's early module loading.
# Make sure these paths are absolutely correct for your system.
os.environ['nnUNet_raw'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
os.environ['nnUNet_preprocessed'] = "/sharedscratch/an252/cancercdetectiondataset/nnUNet_preprocessed" # Corrected typo "cancercdetectiondataset" -> "cancerdetectiondataset"
os.environ['nnUNet_results'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# =======================================================================


import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from batchgenerators.utilities.file_and_folder_operations import load_json

print("Starting TorchScript tracing...")
print("---")
print(f"Checking environment variables (as seen by Python):")
print(f"nnUNet_raw: {os.environ.get('nnUNet_raw')}")
print(f"nnUNet_preprocessed: {os.environ.get('nnUNet_preprocessed')}")
print(f"nnUNet_results: {os.environ.get('nnUNet_results')}")
print("---")
==================== recovery/vscode_history/45yW.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D

# === Paths ===
base_pred = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres"
gt_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
t1ce_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesVal"

outdir = os.path.join(base_pred, "fold_3/hausdorff_visuals_w_mri")
os.makedirs(outdir, exist_ok=True)

# === Class labels ===
class_labels = {1: "NCR", 2: "ED", 3: "ET"}

# === Auto-detect valid patients ===
t1ce_patients = sorted([
    "_".join(f.split("_")[:2])
    for f in os.listdir(t1ce_path_base)
    if f.endswith("_0001.nii.gz")
])
==================== recovery/vscode_history/4Fij.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_dice = brats_dice_df[['dice_wt', 'dice_tc', 'dice_et']].mean().mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)
adv_dice = adv_df[['dice_adv_ET', 'dice_adv_TC', 'dice_adv_WT']].mean().mean()
dice_orig = adv_df[['dice_orig_ET', 'dice_orig_TC', 'dice_orig_WT']].mean().mean()
dice_drop = dice_orig - adv_dice

# visualization setup 
labels = ["BraTS (Train/Val)", "BCBM (Fine-tune/Test)", "Adversarial BraTS"]
modalities = [["T1", "T1ce", "T2", "FLAIR"], ["T1ce", "FLAIR"], ["FGSM", "PGD", "Gaussian"]]
colors = ["#66c2a5", "#fc8d62", "#8da0cb"]
==================== recovery/vscode_history/4FlL.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap


def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_pretty(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with refined aesthetics."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.6, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])
==================== recovery/vscode_history/4OGJ.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats # Import for statistical tests
from tqdm import tqdm # Import for progress bars

print("Starting Adversarial Attack & Defense Script...", flush=True)
print("---", flush=True)

# true to run a brief adversarial fine-tuning session on the black-box model
PERFORM_ADVERSARIAL_TRAINING = True
# a new model 'checkpoint_best_robust.pth' will be created and evaluated.

image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_base_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
output_base_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2"
==================== recovery/vscode_history/4TPR.py ====================
import os
os.environ['nnUNet_raw'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
os.environ['nnUNet_preprocessed'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed" # Corrected typo "cancercdetectiondataset" -> "cancerdetectiondataset"
os.environ['nnUNet_results'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from batchgenerators.utilities.file_and_folder_operations import load_json

print("Starting TorchScript tracing...")
print("---")
print(f"Checking environment variables (as seen by Python):")
print(f"nnUNet_raw: {os.environ.get('nnUNet_raw')}")
print(f"nnUNet_preprocessed: {os.environ.get('nnUNet_preprocessed')}")
print(f"nnUNet_results: {os.environ.get('nnUNet_results')}")
print("---")

checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
save_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/nnunet_model_traced.pt"
plans_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans.json"
dataset_json_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/dataset.json"
os.makedirs(os.path.dirname(save_path), exist_ok=True)
try:
    dataset_json_content = load_json(dataset_json_path)
except FileNotFoundError as e:
    print(f"Error loading JSON files: {e}. Please verify paths: {dataset_json_path} and {plans_path}")
==================== recovery/vscode_history/4Vq7.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy

print("Starting FGSM attack...")

# === Settings ===
patient_id = "BraTS2021_00005"
slice_idx = 75
modality_idx = 1  # 0=T1, 1=T1ce, 2=T2, 3=FLAIR
epsilon = 0.03

# === Paths ===
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"

# === Load nnUNetTrainer
trainer = nnUNetTrainer(plans=None, configuration="3d_fullres", fold=3, dataset_json=None, device='cpu')
trainer.initialize()
checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
==================== recovery/vscode_history/5Ewh.py ====================
#!/usr/bin/env python
import torch
import sys

CHECKPOINT = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/" \
             "Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/" \
             "fold_3/checkpoint_final.pth"
OUTPUT = "braTS_encoder.pth"

# Allow the old numpy scalar global so torch.load works
import torch.serialization
torch.serialization.add_safe_globals([ "numpy._core.multiarray.scalar" ])

# Load the checkpoint (force full unpickling)
ck = torch.load(CHECKPOINT, map_location="cpu", weights_only=False)

# Grab the actual state dict
if isinstance(ck, dict) and "state_dict" in ck:
    sd = ck["state_dict"]
else:
    sd = ck

# Show a few keys so you can see what prefix to use
print(">>> example keys:")
for k in list(sd.keys())[:20]:
==================== recovery/vscode_history/5frW.py ====================
# Place these os.environ assignments AT THE VERY TOP OF YOUR SCRIPT
# before any other imports, especially nnunetv2 imports.
import os

# === Forcefully set nnUNet environment variables at the earliest point ===
# This ensures nnUNetTrainer can find its base paths regardless of shell exports
# or nnunetv2's early module loading.
# Make sure these paths are absolutely correct for your system.
os.environ['nnUNet_raw'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
os.environ['nnUNet_preprocessed'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed" # Corrected typo "cancercdetectiondataset" -> "cancerdetectiondataset"
os.environ['nnUNet_results'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# =======================================================================


import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from batchgenerators.utilities.file_and_folder_operations import load_json

print("Starting TorchScript tracing...")
print("---")
print(f"Checking environment variables (as seen by Python):")
print(f"nnUNet_raw: {os.environ.get('nnUNet_raw')}")
print(f"nnUNet_preprocessed: {os.environ.get('nnUNet_preprocessed')}")
print(f"nnUNet_results: {os.environ.get('nnUNet_results')}")
print("---")
==================== recovery/vscode_history/5ndw.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_with_border(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with overlay and a visible border."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.5, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])

# Training Data Paths
==================== recovery/vscode_history/5wO8.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# --- Data Loading and Processing (same as before) ---
# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_dice = brats_dice_df[['dice_wt', 'dice_tc', 'dice_et']].mean().mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)
adv_dice = adv_df[['dice_adv_ET', 'dice_adv_TC', 'dice_adv_WT']].mean().mean()
dice_orig = adv_df[['dice_orig_ET', 'dice_orig_TC', 'dice_orig_WT']].mean().mean()
dice_drop = dice_orig - adv_dice

# --- Visualization Setup ---
labels = ["BraTS (Train/Val)", "BCBM (Fine-tune/Test)", "Adversarial BraTS"]
modalities = [["T1", "T1ce", "T2", "FLAIR"], ["T1ce", "FLAIR"], ["FGSM", "PGD", "Gaussian"]]
==================== recovery/vscode_history/5zRs.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_with_border(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with overlay and a visible border."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.5, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])

gt_dir          = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
==================== recovery/vscode_history/6Lcq.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random
import warnings
import time

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
==================== recovery/vscode_history/6d65.py ====================
import os
import re
import matplotlib.pyplot as plt
import numpy as np

log_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3"
log_files = [f for f in os.listdir(log_dir) if f.startswith("training_log") and f.endswith(".txt")]
log_files.sort()

train_dice = []
val_dice = []
epoch_durations = []

if not log_files:
    print(f"Error: No training log files found in {log_dir}")
else:
    full_log_content = ""
    for file in log_files:
        with open(os.path.join(log_dir, file), 'r') as f:
            full_log_content += f.read()

    # an epoch block starts with 'Epoch X' and contains the metrics for that epoch.
    # split the entire log by this pattern.
    epoch_blocks = re.split(r'Epoch \d+', full_log_content)

==================== recovery/vscode_history/748Q.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random

import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# ---------------------------------------------------------------------
#  2D U-Net + classifier
# ---------------------------------------------------------------------
class UNet2D(nn.Module):
    def __init__(self, in_ch, base_feats=16, dropout=0.0):
==================== recovery/vscode_history/7G1Y.py ====================
import os
import glob
import nibabel as nib
import numpy as np

print("Script started")

pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/validation"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"

pred_files = sorted(glob.glob(os.path.join(pred_dir, "*.nii.gz")))
print(f"Found {len(pred_files)} predicted files in: {pred_dir}")

for pred_path in pred_files[:3]:
    base = os.path.basename(pred_path).replace(".nii.gz", "")
    gt_file = os.path.join(gt_dir, base + "_0000.nii.gz")

    print(f"‚Üí {base} | GT: {os.path.basename(gt_file)}")

    if not os.path.exists(gt_file):
        print(f"  üö´ Missing ground truth: {gt_file}")
        continue

    pred = nib.load(pred_path).get_fdata() > 0
    gt = nib.load(gt_file).get_fdata() > 0
==================== recovery/vscode_history/7Wbt.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import glob
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import nibabel as nib
from torch.utils.data import Dataset, DataLoader

# -----------------------------------------------------------------------------
#  NOTE ON OOM: 
#  If you hit CUDA OOM again, try one or more of:
#    ‚Ä¢ Lower --batch_size (e.g. 1)
#    ‚Ä¢ Lower --base_feats (e.g. 16)
#    ‚Ä¢ Reduce --target_z (center‚Äêcrop smaller Z)
#    ‚Ä¢ Enable torch.cuda.empty_cache() between epochs
# -----------------------------------------------------------------------------

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
==================== recovery/vscode_history/85wX.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random
import warnings
import time

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
==================== recovery/vscode_history/8Hrq.py ====================
import os
import pandas as pd

# --- 1. SETTINGS (These are the paths we need to verify) ---
dice_csv    = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
image_dir   = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/nnUNetPlans_3d_fullres"
gt_dir      = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir    = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
# ---

# --- 2. SCRIPT TO CHECK FILE EXISTENCE ---
try:
    df = pd.read_csv(dice_csv)
except FileNotFoundError:
    print(f"‚ùå ERROR: Dice scores CSV not found at: {dice_csv}")
    exit()

print("--- Checking file availability for patients in CSV ---\n")

for index, row in df.iterrows():
    # Use the corrected column names 'Patient' and 'Dice'
    patient_id = str(row['Patient']).strip()
    
    # Construct paths
    gt_path = os.path.join(gt_dir, f"{patient_id}.nii.gz")
==================== recovery/vscode_history/8sQW.py ====================
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nibabel as nib
from scipy.spatial.distance import directed_hausdorff
from matplotlib.colors import ListedColormap

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis"
os.makedirs(out_dir, exist_ok=True)

# === Load Data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")

# === Select Top/Bottom 3 Patients ===
==================== recovery/vscode_history/95Zm.py ====================
import os
import time
import nibabel as nib
import numpy as np
import pandas as pd
from sklearn.metrics import f1_score

def dice_score(pred, true):
    pred = pred.flatten()
    true = true.flatten()
    intersection = np.sum(pred * true)
    return 2. * intersection / (np.sum(pred) + np.sum(true) + 1e-6)

pred_folder = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats"
label_root = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/labelsVal"
output_csv = os.path.join(pred_folder, "dicescores_val_brats.csv")

dice_records = []

fold_dirs = sorted([f for f in os.listdir(pred_folder) if f.startswith("fold_") and os.path.isdir(os.path.join(pred_folder, f))])

for fold in fold_dirs:
    fold_path = os.path.join(pred_folder, fold)
    pred_files = [f for f in os.listdir(fold_path) if f.endswith(".nii.gz")]

==================== recovery/vscode_history/9AaS.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer_2d.py

import argparse, os
import torch, torch.nn as nn, torch.optim as optim
import numpy as np, pandas as pd, nibabel as nib
from torch.utils.data import Dataset, DataLoader

# 2D U-Net
class UNet2D(nn.Module):
    def __init__(self, in_ch, base_feats=16):
        super().__init__()
        def block(ic,oc):
            return nn.Sequential(
                nn.Conv2d(ic, oc, 3, padding=1, bias=False),
                nn.BatchNorm2d(oc), nn.ReLU(inplace=True),
                nn.Conv2d(oc, oc, 3, padding=1, bias=False),
                nn.BatchNorm2d(oc), nn.ReLU(inplace=True),
            )
        self.enc1 = block(in_ch, base_feats)
        self.enc2 = block(base_feats, base_feats*2)
        self.enc3 = block(base_feats*2, base_feats*4)
        self.enc4 = block(base_feats*4, base_feats*8)
        self.bot  = block(base_feats*8, base_feats*16)

==================== recovery/vscode_history/9EbG.py ====================
#!/usr/bin/env python
import torch
import torch.serialization

# path to your BraTS nnU-Net checkpoint
CHECKPOINT = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/" \
             "Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/" \
             "fold_3/checkpoint_final.pth"
OUTPUT = "braTS_encoder.pth"

# 1) allow the old numpy scalar global (safe if you trust this checkpoint):
torch.serialization.add_safe_globals([ "numpy._core.multiarray.scalar" ])

# 2) load the full checkpoint (turn off weights_only)
ck = torch.load(CHECKPOINT, map_location="cpu", weights_only=False)

# 3) grab its state_dict
sd = ck.get("state_dict", ck)

# 4) filter for encoder params (they live under network.encoder.*)
encoder_sd = {}
prefix = "network.encoder."
for k, v in sd.items():
    if k.startswith(prefix):
        encoder_sd[k[len(prefix):]] = v
==================== recovery/vscode_history/9SOr.py ====================
#!/usr/bin/env python3
import os
import argparse
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# ----------------------------
# 1) ARGS & CONFIG
# ----------------------------
parser = argparse.ArgumentParser()
parser.add_argument("--mid_slice_dir",
    default="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/mid_slices/",
    help="folder with BCBM_xxxx_T1.png")
parser.add_argument("--metadata_csv",
    default="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv",
    help="csv with nnUNet_ID,HER2_Status")
parser.add_argument("--pretrained_unet",
    required=True,
==================== recovery/vscode_history/9b3L.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap

plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman']
plt.rcParams['mathtext.fontset'] = 'stix'

def find_best_slice(mask_data):
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_pretty(ax, mri_slice, seg_slice, colormap, norm):
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.6, interpolation='none')
    ax.set_xticks([])
==================== recovery/vscode_history/B89A.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D
from sklearn.metrics import f1_score

# === Settings ===
patients = [
    "BraTS2021_00554",
    "BraTS2021_00231",
    "BraTS2021_01220"
]
fold = 3

pred_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/fold_3"
gt_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
img_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesVal"
outdir = os.path.join(pred_base, "hausdorff_slice_visuals_zoomed_by_class")
os.makedirs(outdir, exist_ok=True)

class_labels = {1: "NCR", 2: "ED", 3: "ET"}

==================== recovery/vscode_history/BgjV.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_pretty(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with refined aesthetics."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.6, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])
    for spine in ax.spines.values():
==================== recovery/vscode_history/Bkbt.py ====================
#!/usr/bin/env python
import argparse
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        # encoder
        self.enc1 = self._block(in_ch,       base_feats)
        self.enc2 = self._block(base_feats,  base_feats*2)
        self.enc3 = self._block(base_feats*2,base_feats*4)
        self.enc4 = self._block(base_feats*4,base_feats*8)
        # bottleneck
        self.bot  = self._block(base_feats*8,base_feats*16)
        # decoder
        self.up4  = nn.ConvTranspose3d(base_feats*16, base_feats*8, 2, 2)
        self.dec4 = self._block(base_feats*16, base_feats*8)
        self.up3  = nn.ConvTranspose3d(base_feats*8,  base_feats*4, 2, 2)
        self.dec3 = self._block(base_feats*8,  base_feats*4)
==================== recovery/vscode_history/DF65.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd # Import pandas for CSV output

print("Starting FGSM attack script...")
print("---")

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
# export nnUNet_results="/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# ===========================================================================

==================== recovery/vscode_history/DI0z.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_with_border(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with overlay and a visible border."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.5, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])

gt_dir          = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
==================== recovery/vscode_history/Dqkj.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import time
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from tqdm import tqdm

try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
    TF = None
==================== recovery/vscode_history/E6Zx.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO

print("Starting FGSM attack...")
print("---")

# --- Settings ---
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# --- Paths ---
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

==================== recovery/vscode_history/EMoD.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D

# === Paths ===
base_pred = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres"
gt_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
t1ce_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"


outdir = os.path.join(base_pred, "fold_3/hausdorff_visuals_w_mri")
os.makedirs(outdir, exist_ok=True)

# === Class labels ===
class_labels = {1: "NCR", 2: "ED", 3: "ET"}

# === Auto-detect valid patients ===
t1ce_patients = sorted([
    "_".join(f.split("_")[:2])
    for f in os.listdir(t1ce_path_base)
    if f.endswith("_0001.nii.gz")
==================== recovery/vscode_history/Ed3o.py ====================
import os
import nibabel as nib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.ndimage import zoom
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import cv2
from concurrent.futures import ThreadPoolExecutor

def assign_grade(patient_id):
    try:
        num = int(patient_id.split('_')[1])
    except Exception:
        return "Unknown"
    cutoff = 65  # BraTS 2021 has 65 LGG cases
    return "LGG" if num <= cutoff else "HGG"

# Utility functions
def resample_to_1mm(volume, spacing): 
    resize_factors = [s / 1.0 for s in spacing]
    return zoom(volume, resize_factors, order=1)

==================== recovery/vscode_history/F32L.py ====================
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# === Load Dice Scores ===
dice_path = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
output_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"
os.makedirs(output_dir, exist_ok=True)
df_dice = pd.read_csv(dice_path)

# === Basic Histogram of Dice Scores ===
plt.figure(figsize=(10, 6))
sns.histplot(df_dice["Dice"], bins=20, kde=True)
plt.title("Histogram of Dice Scores (BCBM Fold 3)")
plt.xlabel("Dice Score")
plt.ylabel("Number of Patients")
plt.grid(True)
plt.tight_layout()
plt.savefig("dice_histogram.png")
plt.show()

# === Boxplot of Dice Scores ===
plt.figure(figsize=(6, 4))
sns.boxplot(y=df_dice["Dice"])
==================== recovery/vscode_history/FVL3.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import nibabel as nib
from torch.utils.data import Dataset, DataLoader

# --------------------------------------------
# 3D U‚ÄëNet with segmentation + classification heads
# --------------------------------------------
class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        # encoder blocks
        self.enc1 = self._block(in_ch,    base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
        self.enc4 = self._block(base_feats*4, base_feats*8)
        # bottleneck
==================== recovery/vscode_history/FXeb.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D

# === Settings ===
patients = [
    ("BraTS2021_00554", 0),
    ("BraTS2021_00231", 1),
    ("BraTS2021_01220", 2)
]

base_pred = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats"
gt_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/labelsVal"
image_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesVal"
outdir = os.path.join(base_pred, "hausdorff_slice_visuals_zoomed_by_class")
os.makedirs(outdir, exist_ok=True)

class_labels = {1: "NCR", 2: "ED", 3: "ET"}

def get_surface(mask):
    eroded = binary_erosion(mask)
==================== recovery/vscode_history/FxLH.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nibabel as nib

from matplotlib.colors import ListedColormap
from skimage import measure

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"
os.makedirs(out_dir, exist_ok=True)

# === Load Data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")
sorted_df = merged_df.sort_values(by="Dice", ascending=False).reset_index(drop=True)
==================== recovery/vscode_history/GA1P.py ====================
#!/usr/bin/env python
import argparse
import os

import nibabel as nib
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        # encoder
        self.enc1 = self._block(in_ch, base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
        self.enc4 = self._block(base_feats*4, base_feats*8)
        # bottleneck
        self.bot  = self._block(base_feats*8, base_feats*16)
        # decoder
        self.up4  = nn.ConvTranspose3d(base_feats*16, base_feats*8, 2, 2)
        self.dec4 = self._block(base_feats*16, base_feats*8)
==================== recovery/vscode_history/GcYJ.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats # Import for statistical tests
from tqdm import tqdm # Import for progress bars

print("Starting Adversarial Attack & Defense Script for BCBM Dataset...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script.
# Example:
# export nnUNet_raw="/path/to/your/nnUNet_raw"
# export nnUNet_preprocessed="/path/to/your/nnUNet_preprocessed"
# export nnUNet_results="/path/to/your/nnUNet_results"
==================== recovery/vscode_history/GjB0.py ====================
import os
import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer

# === Paths ===
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
save_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/nnunet_model_traced.pt"

# === Make sure the output directory exists
os.makedirs(os.path.dirname(save_path), exist_ok=True)

# === Load model from checkpoint
checkpoint = torch.load(checkpoint_path, map_location='cpu')
trainer = nnUNetTrainer(plans=None, configuration="3d_fullres", fold=3, dataset_json=None, device='cpu')
trainer.initialize()
trainer.network.load_state_dict(checkpoint['network_weights'])

# === Dummy input
example_input = torch.randn(1, 4, 128, 128, 128)

# === Trace and save
traced_model = torch.jit.trace(trainer.network, example_input)
torch.jit.save(traced_model, save_path)

print(f"‚úÖ TorchScript model saved to:\n{save_path}")
==================== recovery/vscode_history/H4NA.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D

# === Settings ===
patients = [
    ("BraTS2021_00000", 3),
    ("BraTS2021_00005", 3),
    ("BraTS2021_00006", 3),
]

# Paths
base_pred = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats"
gt_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
t1ce_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesVal"

outdir = os.path.join(base_pred, "hausdorff_visuals_w_mri")
os.makedirs(outdir, exist_ok=True)

class_labels = {1: "NCR", 2: "ED", 3: "ET"}

==================== recovery/vscode_history/HBRK.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats # Import for statistical tests
from tqdm import tqdm # Import for progress bars

print("Starting Adversarial Attack & Defense Script...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
==================== recovery/vscode_history/HPzp.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats # Import for statistical tests
from tqdm import tqdm # Import for progress bars

print("Starting Adversarial Attack & Defense Script...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
==================== recovery/vscode_history/HyOz.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json # Import load_json

print("Starting FGSM attack...")

# === Settings ===
patient_id = "BraTS2021_00005"
slice_idx = 75
modality_idx = 1  # 0=T1, 1=T1ce, 2=T2, 3=FLAIR
epsilon = 0.03 # Perturbation strength for FGSM

# === Paths ===
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

# === Set the correct path to your nnUNetPlans.json file ===
# THIS IS THE CORRECTED PATH based on your feedback.
==================== recovery/vscode_history/IWvP.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random

import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# ---------------------------------------------------------------------
#  2D U-Net + classifier
# ---------------------------------------------------------------------
class UNet2D(nn.Module):
    def __init__(self, in_ch, base_feats=16, dropout=0.0):
==================== recovery/vscode_history/IdUn.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()
bcbm_hd95 = bcbm_df['HD95'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_hd_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/hausdorff_sample.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_hd_df = pd.read_csv(brats_hd_path)

brats_dice = brats_dice_df[['ET', 'TC', 'WT']].mean().mean()
brats_hd95 = brats_hd_df[['ET', 'TC', 'WT']].mean().mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)
adv_dice = adv_df[['ET_dice', 'TC_dice', 'WT_dice']].mean().mean()
adv_hd95 = adv_df[['ET_hd95', 'TC_hd95', 'WT_hd95']].mean().mean()

==================== recovery/vscode_history/IhqZ.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random

import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
    TF = None
==================== recovery/vscode_history/Irws.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()
bcbm_hd95 = bcbm_df['HD95'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_hd_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/hausdorff_sample.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_hd_df = pd.read_csv(brats_hd_path)

brats_dice = brats_dice_df[['ET', 'TC', 'WT']].mean().mean()
brats_hd95 = brats_hd_df[['ET', 'TC', 'WT']].mean().mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)
adv_dice = adv_df[['ET_dice', 'TC_dice', 'WT_dice']].mean().mean()
adv_hd95 = adv_df[['ET_hd95', 'TC_hd95', 'WT_hd95']].mean().mean()

==================== recovery/vscode_history/Iw2q.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats
from tqdm import tqdm
import random

print("Starting Adversarial Attack & Defense Script for BCBM Dataset...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnU-Net Environment Variables
# These variables MUST be set in your shell environment *before* running this script.
# ===========================================================================

# --- Configuration ---
==================== recovery/vscode_history/J0pE.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()
bcbm_hd95 = bcbm_df['Hausdorff'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_hd_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/hausdorff_sample.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_hd_df = pd.read_csv(brats_hd_path)

brats_dice = brats_dice_df[['dice_wt', 'dice_tc', 'dice_et']].mean().mean()
brats_hd95 = brats_hd_df['hausdorff_distance'].mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)

adv_dice = adv_df[['dice_adv_ET', 'dice_adv_TC', 'dice_adv_WT']].mean().mean()
dice_orig = adv_df[['dice_orig_ET', 'dice_orig_TC', 'dice_orig_WT']].mean().mean()
==================== recovery/vscode_history/JAzB.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import time
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
    TF = None
    InterpolationMode = None
==================== recovery/vscode_history/JCRg.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap

plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman']
plt.rcParams['mathtext.fontset'] = 'stix'

def find_best_slice(mask_data):
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_pretty(ax, mri_slice, seg_slice, colormap, norm):
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.6, interpolation='none')
    ax.set_xticks([])
==================== recovery/vscode_history/JOtl.py ====================
#!/usr/bin/env python
import torch
import sys

CHECKPOINT = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/" \
             "Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/" \
             "fold_3/checkpoint_final.pth"
OUTPUT = "braTS_encoder.pth"

# Allow the old numpy scalar global so torch.load works
import torch.serialization
torch.serialization.add_safe_globals([ "numpy._core.multiarray.scalar" ])

# load full checkpoint
ck = torch.load(CHECKPOINT, map_location="cpu", weights_only=False)

# nnU-Net packs everything under 'network_weights'
if "network_weights" in ck:
    sd = ck["network_weights"]
else:
    print("ERROR: no 'network_weights' field found in checkpoint")
    sys.exit(1)

print(">>> example keys inside network_weights:")
for k in list(sd.keys())[:20]:
==================== recovery/vscode_history/JTS1.py ====================
import os
import json
import matplotlib.pyplot as plt


log_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3"
json_path = os.path.join(log_dir, 'debug.json')

if not os.path.exists(json_path):
    print(f"Error: debug.json not found in {log_dir}")
else:
    with open(json_path, 'r') as f:
        data = json.load(f)

    # extract data from the JSON file
    # training loss in debug.json is a combined loss (e.g., Dice + Cross-Entropy).
    train_losses = data.get('train_losses')
    
    # the 'val_eval_criterion' key holds the validation Dice scores.
    val_dice = data.get('val_eval_criterion')
    
    # the 'epoch_times' key holds the duration of each epoch.
    epoch_durations = data.get('epoch_times')
    
    # check if data was successfully extracted before proceeding
==================== recovery/vscode_history/JWn7.py ====================
import os
import glob
import nibabel as nib
import numpy as np

print("Script started")

pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"

pred_files = sorted(glob.glob(os.path.join(pred_dir, "*.nii.gz")))
print(f"Found {len(pred_files)} predicted files in: {pred_dir}")

for pred_path in pred_files[:3]:
    base = os.path.basename(pred_path).replace(".nii.gz", "")
    gt_file = os.path.join(gt_dir, base + "_0000.nii.gz")

    print(f"‚Üí {base} | GT: {os.path.basename(gt_file)}")

    if not os.path.exists(gt_file):
        print(f"  üö´ Missing ground truth: {gt_file}")
        continue

    pred = nib.load(pred_path).get_fdata() > 0
    gt = nib.load(gt_file).get_fdata() > 0
==================== recovery/vscode_history/JXsm.py ====================
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter
from sklearn.metrics import roc_auc_score, roc_curve
import seaborn as sns

output_dir = "/sharedscratch/an252/cancerdetectiondataset/output/brats"
os.makedirs(output_dir, exist_ok=True)
metadata_path = os.path.join(output_dir, "preprocessing_metadata_with_splits.csv")
df = pd.read_csv(metadata_path)

assert "PatientID" in df.columns, "Metadata must include 'PatientID'"
assert "Grade" in df.columns, "Metadata must include 'Grade' (HGG or LGG)"

df["Subgroup"] = df["Grade"].map(lambda x: "HGG" if x == "HGG" else "LGG")

dice_scores = []
volume_bins = []
patient_ids = []
nonzero_voxels = []

for _, row in df.iterrows():
    pid = row["PatientID"]
==================== recovery/vscode_history/JoFg.py ====================
#!/usr/bin/env python
import os

# 1) point nnU-Net at your folders using the v2 var names *before* any nnUNet import
os.environ["nnUNet_raw"]         = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
os.environ["nnUNet_preprocessed"]= "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
os.environ["nnUNet_results"]     = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"

import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer

# 2) your paths
PLANS_DIR    = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans_3d_fullres"
DATASET_JSON = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/dataset.json"
CHECKPOINT   = (
    "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/"
    "Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_final.pth"
)
FOLD   = 3
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 3) instantiate trainer (positional args!)
trainer = nnUNetTrainer(
    PLANS_DIR,
    "3d_fullres",
==================== recovery/vscode_history/Js99.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats
from tqdm import tqdm

# --- Optional Visualization Imports ---
try:
    from torchsummary import summary
except ImportError:
    print("torchsummary not found. To enable visualization, please install it: pip install torchsummary")
    summary = None # Allows the script to run without this library
try:
    from torchviz import make_dot
except ImportError:
    print("torchviz not found. To enable visualization, please install it: pip install torchviz")
==================== recovery/vscode_history/K6Xy.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random
import warnings

import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
==================== recovery/vscode_history/KQzb.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO

print("Starting FGSM attack...")
print("---")

# --- Settings ---
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# --- Paths ---
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

==================== recovery/vscode_history/Kipe.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd # Import pandas for CSV output

print("Starting FGSM attack script...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
# export nnUNet_results="/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# export nnUNet_compile=False # Disable torch.compile for checkpoint loading compatibility
# ===========================================================================
==================== recovery/vscode_history/M7W8.py ====================
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# === Paths ===
dice_path = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
output_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"
os.makedirs(output_dir, exist_ok=True)

# === Load Dice Scores ===
df_dice = pd.read_csv(dice_path)

# === Histogram of Dice Scores ===
plt.figure(figsize=(10, 6))
sns.histplot(df_dice["Dice"], bins=20, kde=True)
plt.title("Histogram of Dice Scores (BCBM Fold 3)")
plt.xlabel("Dice Score")
plt.ylabel("Number of Patients")
plt.grid(True)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, "dice_histogram.png"))
plt.close()

# === Boxplot of Dice Scores ===
==================== recovery/vscode_history/MkvM.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse, os, re
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import nibabel as nib
from torch.utils.data import Dataset, DataLoader

# -----------------------------------------------------------------------------
# NOTE ON MEMORY:
# ‚Ä¢ Defaults: base_feats=16, target_z=128, batch_size=1 to fit ~16‚Äì24‚ÄØGB GPU.
# ‚Ä¢ Uses mixed‚Äëprecision (AMP) to halve peak memory.
# ‚Ä¢ If you still OOM: lower base_feats (8), target_z (64), or batch_size (1).
# -----------------------------------------------------------------------------

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=16):
        super().__init__()
        self.enc1 = self._block(in_ch,      base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
==================== recovery/vscode_history/MsBx.py ====================
import numpy as np, nibabel as nib, glob, os

def dice_score(pred, gt):
    intersection = np.sum(pred * gt)
    return 2.0 * intersection / (np.sum(pred) + np.sum(gt) + 1e-8)

gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/validation"

for pred_path in sorted(glob.glob(os.path.join(pred_dir, "*.nii.gz"))):
    case_id = os.path.basename(pred_path)
    gt_path = os.path.join(gt_dir, case_id)
    
    if os.path.exists(gt_path):
        pred = nib.load(pred_path).get_fdata() > 0
        gt = nib.load(gt_path).get_fdata() > 0
        dice = dice_score(pred, gt)
        print(f"{case_id}: Dice = {dice:.4f}")
    else:
        print(f"Missing GT for {case_id}")
==================== recovery/vscode_history/ORyI.py ====================
import json

input_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/dataset.json"
output_path = "/sharedscratch/an252/temp_dataset_fixed.json"
with open(input_path, "r") as f:
    data = json.load(f)

data["labels"] = {str(k): v for k, v in data["labels"].items()}
with open(output_path, "w") as f:
    json.dump(data, f, indent=4)

print(f"Fully fixed JSON written to: {output_path}")
==================== recovery/vscode_history/OW4P.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    #finds the slice index with the largest tumor area for each view.
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_with_border(ax, mri_slice, seg_slice, colormap, norm):
    """plots a single slice with overlay and a visible border."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.5, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])

gt_dir          = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
==================== recovery/vscode_history/OcbG.py ====================
import numpy as np, nibabel as nib, os, glob

def dice_score(pred, gt):
    intersection = np.sum(pred * gt)
    return 2. * intersection / (np.sum(pred) + np.sum(gt) + 1e-8)

gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/validation"

for pred_path in sorted(glob.glob(os.path.join(pred_dir, "*.nii.gz"))):
    case_id = os.path.basename(pred_path)
    gt_path = os.path.join(gt_dir, case_id.replace('.nii.gz', '_0000.nii.gz'))

    if not os.path.exists(gt_path):
        print(f"[MISSING GT] {gt_path}")
        continue

    pred = nib.load(pred_path).get_fdata() > 0
    gt = nib.load(gt_path).get_fdata() > 0

    print(f"[INFO] {case_id}: pred unique = {np.unique(pred)}, gt unique = {np.unique(gt)}")

    if np.sum(gt) == 0 and np.sum(pred) == 0:
        print(f"{case_id}: Dice = N/A (both empty)")
    else:
==================== recovery/vscode_history/Opyg.py ====================
import os
import numpy as np
import pandas as pd
import nibabel as nib
from glob import glob
from tqdm import tqdm


gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/labelsTr"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats"
output_csv_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
pred_files = glob(os.path.join(pred_dir, "fold_*", "*.nii.gz"))
if not pred_files:
    pred_files = glob(os.path.join(pred_dir, "*.nii.gz"))
print(f"Found {len(pred_files)} prediction files.")

def dice_score(gt, pred):
    gt = gt > 0
    pred = pred > 0
    
    intersection = np.sum(gt & pred)
    total = np.sum(gt) + np.sum(pred)
    
    if total == 0:
        # Both masks are empty, perfect agreement
==================== recovery/vscode_history/QMy1.py ====================
import numpy as np, nibabel as nib, os, glob

def dice_score(pred, gt):
    intersection = np.sum(pred * gt)
    return 2. * intersection / (np.sum(pred) + np.sum(gt) + 1e-8)

gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/validation"

for pred_path in sorted(glob.glob(os.path.join(pred_dir, "*.nii.gz"))):
    case_id = os.path.basename(pred_path)
    gt_path = os.path.join(gt_dir, case_id.replace('.nii.gz', '_0000.nii.gz'))

    if not os.path.exists(gt_path):
        print(f"[MISSING GT] {gt_path}")
        continue

    pred = nib.load(pred_path).get_fdata() > 0
    gt = nib.load(gt_path).get_fdata() > 0

    print(f"[INFO] {case_id}: pred unique = {np.unique(pred)}, gt unique = {np.unique(gt)}")

    if np.sum(gt) == 0 and np.sum(pred) == 0:
        print(f"{case_id}: Dice = N/A (both empty)")
    else:
==================== recovery/vscode_history/QP7I.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import os
import argparse
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        # encoder
        self.enc1 = self._block(in_ch, base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
        self.enc4 = self._block(base_feats*4, base_feats*8)
        # bottleneck
        self.bot  = self._block(base_feats*8, base_feats*16)
        # decoder
        self.up4  = nn.ConvTranspose3d(base_feats*16, base_feats*8, 2, 2)
==================== recovery/vscode_history/QSz6.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nibabel as nib

from matplotlib.colors import ListedColormap

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/gt_flat"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"
os.makedirs(out_dir, exist_ok=True)

# === Load Data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")

# === Plot 1: Histogram ===
==================== recovery/vscode_history/QXHi.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO

print("Starting FGSM attack...")
print("---")

# --- Settings ---
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# --- Paths ---
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

==================== recovery/vscode_history/Qcgy.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()
bcbm_hd95 = bcbm_df['Hausdorff'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_hd_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/hausdorff_sample.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_hd_df = pd.read_csv(brats_hd_path)

brats_dice = brats_dice_df[['dice_wt', 'dice_tc', 'dice_et']].mean().mean()
brats_hd95 = brats_hd_df['hausdorff_distance'].mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)
adv_dice = adv_df[['ET_dice', 'TC_dice', 'WT_dice']].mean().mean()
adv_hd95 = adv_df[['ET_hd95', 'TC_hd95', 'WT_hd95']].mean().mean()

==================== recovery/vscode_history/Qcvw.py ====================
import os
import numpy as np
import pandas as pd
import nibabel as nib
from medpy.metric import binary
import time

base_pred_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats"
base_gt_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/labelsVal"
output_csv = os.path.join(base_pred_path, "hausdorff_sample.csv")

results = []

# folds 0 to 4
for fold in range(5):
    fold_path = os.path.join(base_pred_path, f"fold_{fold}")
    if not os.path.isdir(fold_path):
        continue

    count = 0
    for fname in sorted(os.listdir(fold_path)):
        if not fname.endswith(".nii.gz"):
            continue

        patient_id = fname.replace(".nii.gz", "")
==================== recovery/vscode_history/QnUP.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D

# === Settings ===
patients = [
    ("BraTS2021_00554", 0),
    ("BraTS2021_00231", 1),
    ("BraTS2021_01220", 2)
]

base_pred = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats"
gt_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/labelsVal"
image_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesVal"
outdir = os.path.join(base_pred, "hausdorff_slice_visuals_zoomed_by_class")
os.makedirs(outdir, exist_ok=True)

class_labels = {1: "NCR", 2: "ED", 3: "ET"}

def get_surface(mask):
    eroded = binary_erosion(mask)
==================== recovery/vscode_history/R2lF.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()
bcbm_hd95 = bcbm_df['Hausdorff'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_hd_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/hausdorff_sample.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_hd_df = pd.read_csv(brats_hd_path)

brats_dice = brats_dice_df[['dice_wt', 'dice_tc', 'dice_et']].mean().mean()
brats_hd95 = brats_hd_df['hausdorff_distance'].mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)
adv_dice = adv_df[['dice_adv_ET', 'dice_adv_TC', 'dice_adv_WT']].mean().mean()
adv_hd95 = adv_df[['ET_hd95', 'TC_hd95', 'WT_hd95']].mean().mean()

==================== recovery/vscode_history/R8Dg.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nibabel as nib

from matplotlib.colors import ListedColormap
from skimage import measure

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"
os.makedirs(out_dir, exist_ok=True)

# === Load Data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")
sorted_df = merged_df.sort_values(by="Dice", ascending=False).reset_index(drop=True)
==================== recovery/vscode_history/RH5K.py ====================
from .nnUNetTrainer_MultiTask import nnUNetTrainer_MultiTask
==================== recovery/vscode_history/Rbfl.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd # Import pandas for CSV output
import seaborn as sns # Import seaborn for nicer plots

print("Starting FGSM attack script...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
# export nnUNet_results="/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# export nnUNet_compile=False # Disable torch.compile for checkpoint loading compatibility
==================== recovery/vscode_history/S9Is.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import warnings
import time

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
==================== recovery/vscode_history/SkGu.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer.py

import argparse, os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# ---------------------------------------------------------------------
#  2D U-Net + classifier
# ---------------------------------------------------------------------
class UNet2D(nn.Module):
    def __init__(self, in_ch, base_feats=16):
        super().__init__()
        def block(ic, oc):
            return nn.Sequential(
                nn.Conv2d(ic, oc, 3, padding=1, bias=False),
                nn.BatchNorm2d(oc), nn.ReLU(inplace=True),
==================== recovery/vscode_history/SrAp.py ====================
import os
import re
import matplotlib.pyplot as plt
import numpy as np

log_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3"
log_files = [f for f in os.listdir(log_dir) if f.startswith("training_log") and f.endswith(".txt")]
log_files.sort()

train_dice = []
val_dice = []

if not log_files:
    print(f"Error: No training log files found in {log_dir}")
else:
    full_log_content = ""
    for file in log_files:
        with open(os.path.join(log_dir, file), 'r') as f:
            full_log_content += f.read()

    #split the content by epoch blocks
    # an epoch block starts with 'Epoch X' and contains the metrics
    epoch_blocks = re.split(r'Epoch \d+', full_log_content)

    for block in epoch_blocks:
==================== recovery/vscode_history/TeYY.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats
from tqdm import tqdm

# --- Optional Visualization Imports ---
try:
    from torchsummary import summary
except ImportError:
    print("torchsummary not found. To enable visualization, please install it: pip install torchsummary")
    summary = None # Allows the script to run without this library
try:
    from torchviz import make_dot
except ImportError:
    print("torchviz not found. To enable visualization, please install it: pip install torchviz")
==================== recovery/vscode_history/U4bt.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO # For reading raw data with proper affine/spacing

print("Starting FGSM attack...")

# === Settings ===
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# === Paths ===
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

# === Set the correct paths ===
plans_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans.json"
==================== recovery/vscode_history/U8N9.py ====================
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nibabel as nib
from scipy.spatial.distance import directed_hausdorff
from matplotlib.colors import ListedColormap

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis"
os.makedirs(out_dir, exist_ok=True)

# === Load Data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")

# === Hausdorff Distance ===
==================== recovery/vscode_history/UAy4.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json # Import load_json

print("Starting FGSM attack...")

# === Settings ===
patient_id = "BraTS2021_00005"
slice_idx = 75
modality_idx = 1  # 0=T1, 1=T1ce, 2=T2, 3=FLAIR
epsilon = 0.03 # Perturbation strength for FGSM

# === Paths ===
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

# === Set the correct path to your nnUNetPlans.json file ===
plans_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/nnUNetPlans.json" # Adjusted based on common nnUNet output
==================== recovery/vscode_history/UBMw.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nibabel as nib


dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"

# Create the output directory if it doesn't exist
os.makedirs(out_dir, exist_ok=True)

# === Load and Prepare Data ===
try:
    dice_df = pd.read_csv(dice_csv)
    metadata_df = pd.read_csv(metadata_csv)
except FileNotFoundError as e:
    print(f"Error: {e}. Please ensure your file paths in the 'User Configuration' section are correct.")
    # Exit if essential data is missing
==================== recovery/vscode_history/UG1T.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random
import warnings
import time

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
==================== recovery/vscode_history/URi6.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk # Explicitly import SimpleITK for resampling
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
# We no longer need nnUNetPredictor for preprocessing the input tensor for the attack
# from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor

print("Starting FGSM attack...")
print("---")

# --- Settings ---
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# --- Paths ---
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
==================== recovery/vscode_history/UgBt.py ====================
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nibabel as nib
from skimage import measure

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis"
os.makedirs(out_dir, exist_ok=True)

# === Load and merge data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})

merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")

hausdorff_csv = os.path.join(out_dir, "dice_with_hausdorff.csv")
if os.path.exists(hausdorff_csv):
==================== recovery/vscode_history/Uoe6.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"

# Create the output directory if it doesn't exist
os.makedirs(out_dir, exist_ok=True)

# === Load and Prepare Data ===
try:
    dice_df = pd.read_csv(dice_csv)
    metadata_df = pd.read_csv(metadata_csv)
except FileNotFoundError as e:
    print(f"Error: {e}. Please ensure your file paths in the 'User Configuration' section are correct.")
    # Exit if essential data is missing
    exit()

==================== recovery/vscode_history/UxCH.py ====================
import os
import pandas as pd
import numpy as np
import torch
from torch import nn, optim
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from sklearn.model_selection import train_test_split
from tqdm import tqdm

mid_slice_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/mid_slices/"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
output_base = "/sharedscratch/an252/cancerdetectiondataset"
os.makedirs(output_base, exist_ok=True)
df = pd.read_csv(metadata_csv)
df = df[df["HER2_Status"].isin(["+", "-"])].copy()
df["label"] = df["HER2_Status"].map({"+": 1, "-": 0})

image_paths = []
labels = []

for _, row in df.iterrows():
    case_id = row["nnUNet_ID"]
    img_path = os.path.join(mid_slice_dir, f"{case_id}.png")
==================== recovery/vscode_history/V7qU.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy

print("Starting FGSM attack...")

# === Settings ===
patient_id = "BraTS2021_00005"
slice_idx = 75
modality_idx = 1  # 0=T1, 1=T1ce, 2=T2, 3=FLAIR
epsilon = 0.03

# === Paths ===
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"

# === Load nnUNetTrainer
trainer = nnUNetTrainer(plans=None, configuration="3d_fullres", fold=3, dataset_json=None, device=torch.device('cpu'))
trainer.initialize() # <--- This line should be separate
checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
==================== recovery/vscode_history/VWjH.py ====================
import os
import re
import matplotlib.pyplot as plt

# Directory where your logs are stored
log_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3"
log_files = [f for f in os.listdir(log_dir) if f.startswith("training_log") and f.endswith(".txt")]

# Sort log files to process them in order
log_files.sort()

train_dice = []
val_dice = []

# Parse all log files
for file in log_files:
    with open(os.path.join(log_dir, file), 'r') as f:
        for line in f:
            match = re.search(r'train dice:\s*([0-9.]+)\s*val dice:\s*([0-9.]+)', line)
            if match:
                train_dice.append(float(match.group(1)))
                val_dice.append(float(match.group(2)))

# Create output folder
plot_dir = os.path.join(log_dir, "plots")
==================== recovery/vscode_history/Vgrr.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse, os
import torch, torch.nn as nn, torch.optim as optim
import numpy as np, pandas as pd, nibabel as nib
from torch.utils.data import Dataset, DataLoader

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        def block(ic,oc):
            return nn.Sequential(
                nn.Conv3d(ic, oc, 3, padding=1, bias=False),
                nn.BatchNorm3d(oc), nn.ReLU(inplace=True),
                nn.Conv3d(oc, oc, 3, padding=1, bias=False),
                nn.BatchNorm3d(oc), nn.ReLU(inplace=True),
            )
        self.enc1 = block(in_ch, base_feats)
        self.enc2 = block(base_feats, base_feats*2)
        self.enc3 = block(base_feats*2, base_feats*4)
        self.enc4 = block(base_feats*4, base_feats*8)
        self.bot  = block(base_feats*8, base_feats*16)
        self.up4, self.dec4 = nn.ConvTranspose3d(base_feats*16, base_feats*8,2,2), block(base_feats*16, base_feats*8)
        self.up3, self.dec3 = nn.ConvTranspose3d(base_feats*8,  base_feats*4,2,2), block(base_feats*8,  base_feats*4)
==================== recovery/vscode_history/VlWi.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nibabel as nib

# === User Configuration ===
# Please update these paths to match the locations of your files.
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"

# Create the output directory if it doesn't exist
os.makedirs(out_dir, exist_ok=True)

# === Load and Prepare Data ===
try:
    dice_df = pd.read_csv(dice_csv)
    metadata_df = pd.read_csv(metadata_csv)
except FileNotFoundError as e:
    print(f"Error: {e}. Please ensure your file paths in the 'User Configuration' section are correct.")
==================== recovery/vscode_history/Vt0Y.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import warnings
import time

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
==================== recovery/vscode_history/VxNL.py ====================
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nibabel as nib
from skimage import measure
from matplotlib.colors import ListedColormap

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis"
os.makedirs(out_dir, exist_ok=True)

# === Load data and select top 3 patients ===
df = pd.read_csv(os.path.join(out_dir, "dice_with_hausdorff.csv"))
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
df = pd.merge(df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")

top3 = df.sort_values("Dice", ascending=False).head(3)

==================== recovery/vscode_history/VxQl.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_pretty(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with refined aesthetics."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.6, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])
    for spine in ax.spines.values():
==================== recovery/vscode_history/WGsF.py ====================
import os
import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer

# 1) nnUNet paths must be set in the ENV (see above) or you can set them here:
os.environ["nnUNet_raw_data_base"]    = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
os.environ["nnUNet_preprocessed"]     = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
os.environ["nnUNet_results"]          = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"

# 2) point to the 3d_fullres plans and your dataset.json
PLANS_DIR      = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans_3d_fullres"
DATASET_JSON   = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/dataset.json"
CHECKPOINT     = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_final.pth"
FOLD           = 3
DEVICE         = "cuda"

# 3) instantiate the trainer
trainer = nnUNetTrainer(
    plans_file=PLANS_DIR,
    configuration="3d_fullres",
    fold=FOLD,
    dataset_json=DATASET_JSON,
    device=DEVICE
)
trainer.initialize(training=False)  # only need weights
==================== recovery/vscode_history/WNSH.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py (2D version)

import argparse, os, torch
import torch.nn as nn, torch.optim as optim
import numpy as np, pandas as pd, nibabel as nib
from torch.utils.data import Dataset, DataLoader

class UNet2D(nn.Module):
    def __init__(self, in_ch, base_feats=16):  # reduced default for 2D
        super().__init__()
        def block(ic, oc):
            return nn.Sequential(
                nn.Conv2d(ic, oc, 3, padding=1, bias=False),
                nn.BatchNorm2d(oc), nn.ReLU(inplace=True),
                nn.Conv2d(oc, oc, 3, padding=1, bias=False),
                nn.BatchNorm2d(oc), nn.ReLU(inplace=True),
            )
        self.enc1 = block(in_ch,      base_feats)
        self.enc2 = block(base_feats, base_feats*2)
        self.enc3 = block(base_feats*2, base_feats*4)
        self.enc4 = block(base_feats*4, base_feats*8)
        self.bot  = block(base_feats*8, base_feats*16)

        self.up4, self.dec4 = nn.ConvTranspose2d(base_feats*16, base_feats*8, 2, 2), block(base_feats*16, base_feats*8)
==================== recovery/vscode_history/Wanh.py ====================
import os
import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from batchgenerators.utilities.file_and_folder_operations import load_json

# === Forcefully set nnUNet environment variables for this script execution ===
# This ensures nnUNetTrainer can find its base paths regardless of shell exports.
# Make sure these paths are absolutely correct for your system.
os.environ['nnUNet_raw'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
os.environ['nnUNet_preprocessed'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
os.environ['nnUNet_results'] = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# ===========================================================================

print("Starting TorchScript tracing...")
print("---")
print(f"Checking environment variables (after setting internally):")
print(f"nnUNet_raw: {os.environ.get('nnUNet_raw')}")
print(f"nnUNet_preprocessed: {os.environ.get('nnUNet_preprocessed')}")
print(f"nnUNet_results: {os.environ.get('nnUNet_results')}")
print("---")


# === Paths ===
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
save_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/nnunet_model_traced.pt"
==================== recovery/vscode_history/Ws4e.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import warnings
import time

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
==================== recovery/vscode_history/XSzI.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse, os, torch
import torch.nn as nn, torch.optim as optim
import numpy as np, pandas as pd, nibabel as nib
from torch.utils.data import Dataset, DataLoader, random_split
import matplotlib.pyplot as plt

# ----------------------------------------
# Your UNet3D and Dataset classes unchanged
# ----------------------------------------

class UNet3D(nn.Module):
    # ... (same as your final 2D‚Äêreduced version)
    def __init__(self, in_ch, base_feats=16):
        super().__init__()
        def block(ic,oc):
            return nn.Sequential(
                nn.Conv3d(ic, oc, 3, padding=1, bias=False),
                nn.BatchNorm3d(oc), nn.ReLU(inplace=True),
                nn.Conv3d(oc, oc, 3, padding=1, bias=False),
                nn.BatchNorm3d(oc), nn.ReLU(inplace=True),
            )
        self.enc1 = block(in_ch,      base_feats)
==================== recovery/vscode_history/XXkA.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nibabel as nib

from matplotlib.colors import ListedColormap

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"
os.makedirs(out_dir, exist_ok=True)

# === Load Data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")

# === Plot 1: Histogram ===
==================== recovery/vscode_history/XZ6v.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json

print("Starting FGSM attack...")

# === Settings ===
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# === Paths ===
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

# === Set the correct path to your nnUNetPlans.json file ===
plans_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans.json"

==================== recovery/vscode_history/YOul.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats
from tqdm import tqdm

# --- Optional Visualization Imports ---
try:
    from torchsummary import summary
except ImportError:
    print("torchsummary not found. To enable visualization, please install it: pip install torchsummary")
    summary = None # Allows the script to run without this library
try:
    from torchviz import make_dot
except ImportError:
    print("torchviz not found. To enable visualization, please install it: pip install torchviz")
==================== recovery/vscode_history/YQTh.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse, os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import nibabel as nib
from torch.utils.data import Dataset, DataLoader

# -----------------------------------------------------------------------------
# NOTE ON MEMORY:
# ‚Ä¢ We default base_feats=16 and target_z=128 to keep you under ~24 GB.
# ‚Ä¢ Mixed-precision (amp) will cut your peak by ~50%.
# ‚Ä¢ If you still OOM: drop base_feats to 8, target_z to 64, or batch_size to 1.
# -----------------------------------------------------------------------------

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=16):
        super().__init__()
        self.enc1 = self._block(in_ch,     base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
==================== recovery/vscode_history/YUmY.py ====================
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nibabel as nib
from skimage import measure
from matplotlib.colors import ListedColormap

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis"
os.makedirs(out_dir, exist_ok=True)

# === Load data and select top 3 patients ===
df = pd.read_csv(os.path.join(out_dir, "dice_with_hausdorff.csv"))
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
df = pd.merge(df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")
top3 = df.sort_values("Dice", ascending=False).head(3)


==================== recovery/vscode_history/YYrX.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nibabel as nib


dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"

os.makedirs(out_dir, exist_ok=True)

try:
    dice_df = pd.read_csv(dice_csv)
    metadata_df = pd.read_csv(metadata_csv)
except FileNotFoundError as e:
    print(f"Error: {e}. Please ensure your file paths in the 'User Configuration' section are correct.")
    exit()

metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
==================== recovery/vscode_history/YjYb.py ====================
#!/usr/bin/env python
import os

# 1) point nnU-Net at your folders using the v2 var names *before* any nnUNet import
os.environ["nnUNet_raw"]         = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
os.environ["nnUNet_preprocessed"]= "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
os.environ["nnUNet_results"]     = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"

import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer

# 2) your paths
PLANS_DIR    = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json"
DATASET_JSON = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/dataset.json"
CHECKPOINT   = (
    "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/"
    "Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_final.pth"
)
FOLD   = 3
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 3) instantiate trainer (positional args!)
trainer = nnUNetTrainer(
    PLANS_DIR,
    "3d_fullres",
==================== recovery/vscode_history/Yndi.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse, os, re
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import nibabel as nib
from torch.utils.data import Dataset, DataLoader

# -----------------------------------------------------------------------------
# NOTE ON MEMORY:
# ‚Ä¢ Defaults: base_feats=16, target_z=128, batch_size=1 to fit ~16‚Äì24‚ÄØGB GPU.
# ‚Ä¢ Uses mixed‚Äëprecision (AMP) to halve peak memory.
# ‚Ä¢ If you still OOM: lower base_feats (8), target_z (64), or batch_size (1).
# -----------------------------------------------------------------------------

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=16):
        super().__init__()
        self.enc1 = self._block(in_ch,      base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
==================== recovery/vscode_history/Yu9m.py ====================
# finetune_bcbm.py
import os
import json
import importlib
import torch
import nibabel as nib
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torch import nn
from torch.optim import AdamW
from tqdm import tqdm

DATA_DIR = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM"
GT_DIR = os.path.join(DATA_DIR, "gt_segmentations")
IMG_DIR = os.path.join(DATA_DIR, "imagesTr")  # or  folder with 4D NIfTI images
PLANS_PATH = os.path.join(DATA_DIR, "nnUNetPlans.json")
PRETRAINED_PATH = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_final.pth"
CONFIG = "3d_fullres"
SAVE_PATH = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/finetuned_nnunet_bcbm.pth"
BATCH_SIZE = 1
LR = 1e-4
EPOCHS = 100
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class BCBMDataset(Dataset):
==================== recovery/vscode_history/Z8uD.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D

base_pred = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres"
gt_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
t1ce_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
outdir = os.path.join(base_pred, "fold_3/hausdorff_visuals_w_mri")
os.makedirs(outdir, exist_ok=True)
class_labels = {1: "NCR", 2: "ED", 3: "ET"}

t1ce_patients = sorted([
    "_".join(f.split("_")[:2])
    for f in os.listdir(t1ce_path_base)
    if f.endswith("_0001.nii.gz")
])

valid_patients = []
for pid in t1ce_patients:
    pred_path = os.path.join(base_pred, "fold_3", "validation", f"{pid}.nii.gz")
    gt_path = os.path.join(gt_path_base, f"{pid}.nii.gz")
==================== recovery/vscode_history/bcrX.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO

print("Starting FGSM attack...")
print("---")

# --- Settings ---
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# --- Paths ---
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

==================== recovery/vscode_history/d2jh.py ====================
import os
import glob
import nibabel as nib
import numpy as np

print("Script started")

pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/gt_flat/"

pred_files = sorted(glob.glob(os.path.join(pred_dir, "*.nii.gz")))
print(f"Found {len(pred_files)} predicted files in: {pred_dir}")

for pred_path in pred_files[:3]:
    base = os.path.basename(pred_path).replace(".nii.gz", "")
    gt_file = os.path.join(gt_dir, base + "_0000.nii.gz")

    print(f"‚Üí {base} | GT: {os.path.basename(gt_file)}")

    if not os.path.exists(gt_file):
        print(f"  üö´ Missing ground truth: {gt_file}")
        continue

    pred = nib.load(pred_path).get_fdata() > 0
    gt = nib.load(gt_file).get_fdata() > 0
==================== recovery/vscode_history/dTep.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()
bcbm_hd95 = bcbm_df['Hausdorff'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_hd_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/hausdorff_sample.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_hd_df = pd.read_csv(brats_hd_path)

brats_dice = brats_dice_df[['dice_wt', 'dice_tc', 'dice_et']].mean().mean()
brats_hd95 = brats_hd_df['hausdorff_distance'].mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)

adv_dice = adv_df[['dice_adv_ET', 'dice_adv_TC', 'dice_adv_WT']].mean().mean()
dice_orig = adv_df[['dice_orig_ET', 'dice_orig_TC', 'dice_orig_WT']].mean().mean()
==================== recovery/vscode_history/dpgz.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse, os, torch
import torch.nn as nn, torch.optim as optim
import numpy as np, pandas as pd, nibabel as nib
from torch.utils.data import Dataset, DataLoader

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=16):  # reduced default base_feats to 16
        super().__init__()
        def block(ic,oc):
            return nn.Sequential(
                nn.Conv3d(ic, oc, 3, padding=1, bias=False),
                nn.BatchNorm3d(oc), nn.ReLU(inplace=True),
                nn.Conv3d(oc, oc, 3, padding=1, bias=False),
                nn.BatchNorm3d(oc), nn.ReLU(inplace=True),
            )
        self.enc1 = block(in_ch,      base_feats)
        self.enc2 = block(base_feats, base_feats*2)
        self.enc3 = block(base_feats*2, base_feats*4)
        self.enc4 = block(base_feats*4, base_feats*8)
        self.bot  = block(base_feats*8, base_feats*16)
        self.up4, self.dec4 = nn.ConvTranspose3d(base_feats*16, base_feats*8,2,2), block(base_feats*16, base_feats*8)
        self.up3, self.dec3 = nn.ConvTranspose3d(base_feats*8,  base_feats*4,2,2), block(base_feats*8,  base_feats*4)
==================== recovery/vscode_history/eVBb.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor

print("Starting FGSM attack...")
print("---")

# --- Settings ---
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# --- Paths ---
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

==================== recovery/vscode_history/ebCL.py ====================
import os
import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from batchgenerators.utilities.file_and_folder_operations import load_json

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# DO NOT uncomment and use os.environ here, as it may be too late for nnUNet's internal path resolution.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
# export nnUNet_results="/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# ===========================================================================

print("Starting TorchScript tracing...")
print("---")
print(f"Checking environment variables (as seen by Python):")
print(f"nnUNet_raw: {os.environ.get('nnUNet_raw')}")
print(f"nnUNet_preprocessed: {os.environ.get('nnUNet_preprocessed')}")
print(f"nnUNet_results: {os.environ.get('nnUNet_results')}")
print("---")


==================== recovery/vscode_history/f18d.py ====================
import os
import re
import matplotlib.pyplot as plt
import numpy as np

log_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3"
log_files = [f for f in os.listdir(log_dir) if f.startswith("training_log") and f.endswith(".txt")]
log_files.sort()

train_dice = []
val_dice = []
epoch_durations = []

if not log_files:
    print(f"Error: No training log files found in {log_dir}")
else:
    full_log_content = ""
    for file in log_files:
        with open(os.path.join(log_dir, file), 'r') as f:
            full_log_content += f.read()

    # an epoch block starts with 'Epoch X' and contains the metrics for that epoch.
    # split the entire log by this pattern.
    epoch_blocks = re.split(r'Epoch \d+', full_log_content)

==================== recovery/vscode_history/f59H.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats
from tqdm import tqdm
import random

print("Starting Adversarial Attack & Defense Script for BCBM Dataset...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnU-Net Environment Variables
# These variables MUST be set in your shell environment *before* running this script.
# ===========================================================================

# --- Configuration ---
==================== recovery/vscode_history/fJH5.py ====================
import os
import pandas as pd

dice_csv    = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
image_dir   = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/nnUNetPlans_3d_fullres"
gt_dir      = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir    = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"

try:
    df = pd.read_csv(dice_csv)
except FileNotFoundError:
    print(f" ERROR: Dice scores CSV not found at: {dice_csv}")
    exit()

print("--- Checking file availability for patients in CSV ---\n")

for index, row in df.iterrows():
    patient_id = str(row['Patient']).strip()
    
    gt_path = os.path.join(gt_dir, f"{patient_id}.nii.gz")
    pred_path = os.path.join(pred_dir, f"{patient_id}.nii.gz")
    mri_path = os.path.join(image_dir, f"{patient_id}.npz") # Preprocessed files are .npz

    gt_ok = os.path.exists(gt_path)
    pred_ok = os.path.exists(pred_path)
==================== recovery/vscode_history/fivt.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap

plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman']
plt.rcParams['mathtext.fontset'] = 'stix'

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_pretty(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with refined aesthetics."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
==================== recovery/vscode_history/fj67.py ====================
from .nnUNetTrainer_FullyFrozen import nnUNetTrainer_FullyFrozen
==================== recovery/vscode_history/fjih.py ====================
import os
import json
import shutil

# Paths
split_json = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/splits_final.json"
predictions_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions"
output_base = predictions_dir  # we will create val_brats/fold_0 etc

# Load splits
with open(split_json, "r") as f:
    splits = json.load(f)

# Create folders and move predictions
for fold_idx, split in enumerate(splits):
    fold_dir = os.path.join(output_base, f"fold_{fold_idx}")
    os.makedirs(fold_dir, exist_ok=True)

    for case_id in split["val"]:
        src = os.path.join(predictions_dir, f"{case_id}.nii.gz")
        dst = os.path.join(fold_dir, f"{case_id}.nii.gz")
        if os.path.exists(src):
            shutil.move(src, dst)
        else:
            print(f"[WARNING] Missing prediction: {src}")
==================== recovery/vscode_history/gCwm.py ====================
#!/usr/bin/env python
import os
import torch

# 1) point these at your installation
CHECKPOINT = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/" \
             "Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/" \
             "fold_3/checkpoint_final.pth"
OUTPUT = "braTS_encoder.pth"

# 2) load the checkpoint
ck = torch.load(CHECKPOINT, map_location="cpu")
# nnUNetv2 saves the model under either 'state_dict' or directly at top level
sd = ck.get("state_dict", ck)

# 3) filter out only the encoder parameters
encoder_sd = {}
prefix = "network.encoder."
for k, v in sd.items():
    if k.startswith(prefix):
        # strip the prefix so you can load cleanly into your own U-Net
        newk = k[len(prefix):]
        encoder_sd[newk] = v

# 4) save
==================== recovery/vscode_history/gMPh.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats
from tqdm import tqdm

# --- Optional Visualization Imports ---
try:
    from torchsummary import summary
except ImportError:
    print("torchsummary not found. To enable visualization, please install it: pip install torchsummary")
    summary = None # Allows the script to run without this library
try:
    from torchviz import make_dot
except ImportError:
    print("torchviz not found. To enable visualization, please install it: pip install torchviz")
==================== recovery/vscode_history/gQ8U.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_pretty(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with refined aesthetics."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.6, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])
    for spine in ax.spines.values():
==================== recovery/vscode_history/h1xi.py ====================
#!/bin/bash
#
#SBATCH --job-name=nnunet_multi
#SBATCH --partition=gpu                
#SBATCH --gres=gpu:1                   
#SBATCH --cpus-per-task=8              
#SBATCH --mem=64G                      
#SBATCH --time=2-00:00:00              
#SBATCH --output=/sharedscratch/an252/cancerdetectiondataset/logs/nnunet_multi_%j.out
#SBATCH --error=/sharedscratch/an252/cancerdetectiondataset/logs/nnunet_multi_%j.err

# load your environment
module load anaconda3                 
source activate brats-env             

# point nnU-Net to your data
export nnUNet_raw_data_base=/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw
export nnUNet_preprocessed=/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed
export RESULTS_FOLDER=/sharedscratch/an252/cancerdetectiondataset/nnUNet_results

# run training on fold 3 with your new multitask trainer
nnUNetv2_train 002 3d_fullres 3 nnUNetTrainer_MultiTaskSimple
==================== recovery/vscode_history/hFtp.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_with_border(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with overlay and a visible border."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.6, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])

gt_tr_dir       = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
==================== recovery/vscode_history/hRmi.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nibabel as nib

from matplotlib.colors import ListedColormap
from skimage import measure

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"
os.makedirs(out_dir, exist_ok=True)

# === Load Data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")
sorted_df = merged_df.sort_values(by="Dice", ascending=False).reset_index(drop=True)
==================== recovery/vscode_history/hysc.py ====================
import pandas as pd
import matplotlib.pyplot as plt
import os

# --- Configuration ---
# Correct path to your summary CSV file
file_path = '/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_summary.csv'

# Define the directory where the plot will be saved
output_dir = '/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/figures'
output_filename = 'dice_summary_visualization.png'
output_path = os.path.join(output_dir, output_filename)

try:
    # Read the summary CSV file
    df = pd.read_csv(file_path)

    # --- Data Extraction and Renaming ---
    # The last two rows of the nnU-Net summary contain the mean and std dev
    # Extract the second to last row for mean scores
    mean_scores = df.iloc[-2]
    # Extract the last row for standard deviation
    std_devs = df.iloc[-1]

    # Define the mapping from the CSV column names to your desired names
==================== recovery/vscode_history/i9Nj.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO

print("Starting FGSM attack...")
print("---")

# --- Settings ---
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# --- Paths ---
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

==================== recovery/vscode_history/iA6l.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()
bcbm_hd95 = bcbm_df['Hausdorff'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_hd_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/hausdorff_sample.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_hd_df = pd.read_csv(brats_hd_path)

brats_dice = brats_dice_df[['dice_wt', 'dice_tc', 'dice_et']].mean().mean()
brats_hd95 = brats_hd_df['hausdorff_distance'].mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)

adv_dice = adv_df[['dice_adv_ET', 'dice_adv_TC', 'dice_adv_WT']].mean().mean()
dice_orig = adv_df[['dice_orig_ET', 'dice_orig_TC', 'dice_orig_WT']].mean().mean()
==================== recovery/vscode_history/ixKe.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import nibabel as nib
from torch.utils.data import Dataset, DataLoader

# --------------------------------------------------
# If you still hit OOM: lower --batch_size, --base_feats or --target_z
# --------------------------------------------------

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        self.enc1 = self._block(in_ch, base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
        self.enc4 = self._block(base_feats*4, base_feats*8)
        self.bot  = self._block(base_feats*8, base_feats*16)
==================== recovery/vscode_history/jXXC.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import os
import glob
import argparse
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        # encoder
        self.enc1 = self._block(in_ch, base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
        self.enc4 = self._block(base_feats*4, base_feats*8)
        # bottleneck
        self.bot  = self._block(base_feats*8, base_feats*16)
        # decoder
==================== recovery/vscode_history/k5DE.py ====================
import os
import json

# set base paths
base_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS"
images_dir = os.path.join(base_dir, "imagesTr")
labels_dir = os.path.join(base_dir, "labelsTr")

# updated labels: keys are strings (names), values are integers
labels = {
    "background": 0,
    "edema": 1,
    "non-enhancing tumor core": 2,
    "enhancing tumor": 3
}

# modality/channel names: keys must be strings for valid JSON
modalities = {
    "0": "FLAIR",
    "1": "T1",
    "2": "T1ce",
    "3": "T2"
}

channel_names = modalities.copy()
==================== recovery/vscode_history/k7zg.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO # For reading raw data with proper affine/spacing
from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor # Import nnUNetPredictor

print("Starting FGSM attack...")
print("---")

# --- Settings ---
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# --- Paths ---
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

==================== recovery/vscode_history/mKXF.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random

import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# ---------------------------------------------------------------------
#  2D U-Net + classifier
# ---------------------------------------------------------------------
class UNet2D(nn.Module):
    def __init__(self, in_ch, base_feats=16, dropout=0.0):
==================== recovery/vscode_history/mdsK.py ====================
import torch
from nnunetv2.run.run_training import load_pretrained_trainer  # utility to get trainer
import json

# 1. point to your checkpoint and dataset.json
CHECKPOINT = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/" \
             "nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_final.pth"
DATASET_JSON = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/dataset.json"
PLANS = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans_3d_fullres"

# 2. load the trainer (this builds the network according to the same plans/config)
trainer = load_pretrained_trainer(
    plans_file=PLANS,
    dataset_json=DATASET_JSON,
    trainer_class_name="nnUNetTrainer",
    configuration="3d_fullres",
    fold=3,
    checkpoint=CHECKPOINT,
    device="cpu"  # extract on CPU
)

# 3. grab only the encoder sub-module state_dict
full_sd = trainer.network.state_dict()
enc_sd = {k.replace("encoder.", ""): v
          for k, v in full_sd.items()
==================== recovery/vscode_history/meHF.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import nibabel as nib
from torch.utils.data import Dataset, DataLoader

# -----------------------------------------------------------------------------
#  NOTE ON OOM: 
#  If you hit CUDA OOM, try one or more of:
#    ‚Ä¢ Lower --batch_size (e.g. 1)
#    ‚Ä¢ Lower --base_feats (e.g. 16)
#    ‚Ä¢ Reduce --target_z (center-crop smaller Z)
#    ‚Ä¢ Enable torch.cuda.empty_cache() between epochs
# -----------------------------------------------------------------------------

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
==================== recovery/vscode_history/mwPq.py ====================
import os
import json

os.environ['nnUNet_raw'] = '/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw'
os.environ['nnUNet_preprocessed'] = '/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed'
os.environ['nnUNet_results'] = '/sharedscratch/an252/cancerdetectiondataset/nnUNet_results'

from nnunetv2.paths import nnUNet_results
from nnunetv2.training.nnUNetTrainer import nnUNetTrainer_FrozenEncoderBCBM

print(" Import successful: nnUNetTrainer_FrozenEncoderBCBM")

plans_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans.json"
with open(plans_path, 'r') as f:
    plans = json.load(f)

cfg = plans["configurations"]["3d_fullres"]
arch = cfg["architecture"]["arch_kwargs"]

configuration = {
    "num_input_channels": 4,
    "num_output_classes": 3,
    "unet_class_name": "PlainConvUNet",
    "base_num_features": arch["features_per_stage"][0],
    "num_pool": arch["n_stages"] - 1,
==================== recovery/vscode_history/o6sz.py ====================
import torch
import sys

CHECKPOINT = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/" \
             "Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/" \
             "fold_3/checkpoint_final.pth"
OUTPUT = "braTS_encoder.pth"

import torch.serialization
torch.serialization.add_safe_globals([ "numpy._core.multiarray.scalar" ])
ck = torch.load(CHECKPOINT, map_location="cpu", weights_only=False)
if "network_weights" in ck:
    sd = ck["network_weights"]
else:
    print("ERROR: no 'network_weights' field found in checkpoint")
    sys.exit(1)

print(">>> example keys inside network_weights:")
for k in list(sd.keys())[:20]:
    print("   ", k)
print()
if any(k.startswith("encoder.") for k in sd):
    prefix = "encoder."
elif any(k.startswith("network.encoder.") for k in sd):
    prefix = "network.encoder."
==================== recovery/vscode_history/oCQB.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy

print("Starting FGSM attack...")

# === Settings ===
patient_id = "BraTS2021_00005"
slice_idx = 75
modality_idx = 1  # 0=T1, 1=T1ce, 2=T2, 3=FLAIR
epsilon = 0.03 # Perturbation strength for FGSM

# === Paths ===
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

# === Set the correct path to your nnUNetPlans.json file ===
# This path is confirmed from your previous message.
plans_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans.json"
==================== recovery/vscode_history/oMGj.py ====================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nibabel as nib

from matplotlib.colors import ListedColormap

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesVal_fold3"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/gt_flat"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/figures"
os.makedirs(out_dir, exist_ok=True)

# === Load Data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")

# === Plot 1: Histogram ===
==================== recovery/vscode_history/oSl0.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import time
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from tqdm import tqdm

try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
    TF = None
==================== recovery/vscode_history/oTQ0.py ====================
import pandas as pd
import matplotlib.pyplot as plt
import os


file_path = '/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv'
output_dir = '/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/figures'
output_filename = 'dice_score_visualization.png'
output_path = os.path.join(output_dir, output_filename)

try:
    df = pd.read_csv(file_path)
    identifier_column = df.columns[0]
    dice_scores = df.drop(columns=[identifier_column])
    mean_scores = dice_scores.mean()
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(12, 7))
    colors = plt.cm.viridis(range(len(mean_scores)))
    bars = ax.bar(mean_scores.index, mean_scores.values, color=colors)
    for bar in bars:
        yval = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.3f}', va='bottom', ha='center', fontsize=10)

    ax.set_title('Mean Dice Score per Tumor Class', fontsize=16, fontweight='bold')
    ax.set_xlabel('Tumor Class', fontsize=12)
==================== recovery/vscode_history/oUas.py ====================
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nibabel as nib
from scipy.spatial.distance import directed_hausdorff
from matplotlib.colors import ListedColormap

# === Config ===
dice_csv = "/home/an252/deep-learning-for-cancer-detection/bcbm_fold3_val_dice.csv"
metadata_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/bcbm_metadata.csv"
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset002_BCBM/imagesTr"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset002_BCBM/gt_segmentations"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis"
os.makedirs(out_dir, exist_ok=True)

# === Load Data ===
dice_df = pd.read_csv(dice_csv)
metadata_df = pd.read_csv(metadata_csv)
metadata_df = metadata_df.rename(columns={"nnUNet_ID": "Patient"})
merged_df = pd.merge(dice_df, metadata_df[["Patient", "HER2_Status"]], on="Patient", how="left")

# === Select Top/Bottom 3 Patients ===
==================== recovery/vscode_history/rU3D.py ====================
import os
import pandas as pd

# --- 1. SETTINGS (Please double-check these paths) ---
# Training Data Paths
gt_tr_dir       = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
mri_tr_dir      = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"

# Validation Data Paths
gt_val_dir      = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/labelsVal"
mri_val_dir     = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesVal"

# Prediction and CSV Paths
pred_dir_base   = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres"
dice_scores_csv = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dicescores_val_brats.csv"

# --- 2. SCRIPT TO CHECK FILE EXISTENCE ---
try:
    df = pd.read_csv(dice_scores_csv)
except FileNotFoundError:
    print(f"‚ùå ERROR: Dice scores CSV not found at: {dice_scores_csv}")
    exit()

print("--- Checking file availability for all patients in CSV ---\n")

==================== recovery/vscode_history/royt.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D

# === Settings ===
patients = [
    ("BraTS2021_00000", 3),
    ("BraTS2021_00005", 3),
    ("BraTS2021_00006", 3),
]

# Paths
base_pred = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats"
gt_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
t1ce_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesVal"

outdir = os.path.join(base_pred, "hausdorff_visuals_w_mri")
os.makedirs(outdir, exist_ok=True)

class_labels = {1: "NCR", 2: "ED", 3: "ET"}

==================== recovery/vscode_history/rqNi.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd # For saving results to CSV

print("Starting FGSM attack script...")
print("---")

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
# export nnUNet_results="/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"
# ===========================================================================

==================== recovery/vscode_history/s87G.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random
import warnings
import time

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
==================== recovery/vscode_history/sSyP.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D

patients = [
    ("BraTS2021_00554", 0),
    ("BraTS2021_00231", 1),
    ("BraTS2021_01220", 2)
]

base_pred = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats"
gt_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/labelsVal"
image_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesVal"
outdir = os.path.join(base_pred, "hausdorff_slice_visuals_zoomed_by_class")
os.makedirs(outdir, exist_ok=True)

class_labels = {1: "NCR", 2: "ED", 3: "ET"}

def get_surface(mask):
    eroded = binary_erosion(mask)
    return np.argwhere(mask & ~eroded)
==================== recovery/vscode_history/suUm.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked_debug.py

import argparse
import os
import random
import warnings
import time

import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
==================== recovery/vscode_history/syGM.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import os
import argparse
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        # Encoder
        self.enc1 = self._block(in_ch, base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
        self.enc4 = self._block(base_feats*4, base_feats*8)
        # Bottleneck
        self.bot  = self._block(base_feats*8, base_feats*16)
        # Decoder
        self.up4  = nn.ConvTranspose3d(base_feats*16, base_feats*8, 2, 2)
==================== recovery/vscode_history/t2uR.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import nibabel as nib
from torch.utils.data import Dataset, DataLoader

# -----------------------------------------------------------------------------
#  NOTE ON OOM:
#  If you hit CUDA OOM again, try one or more of:
#    ‚Ä¢ Lower --batch_size (e.g. 1)
#    ‚Ä¢ Lower --base_feats (e.g. 16)
#    ‚Ä¢ Reduce --target_z (center‚Äêcrop smaller Z)
#    ‚Ä¢ Enable torch.cuda.empty_cache() between epochs
# -----------------------------------------------------------------------------

class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
==================== recovery/vscode_history/tJ0h.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import time
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
    TF = None
    InterpolationMode = None
==================== recovery/vscode_history/tUpA.py ====================
import os
import json
import importlib
import argparse
import torch
import nibabel as nib
import numpy as np
from torch.utils.data import Dataset, DataLoader  
from torch import nn
from torch.optim import AdamW

parser = argparse.ArgumentParser(description="2D slice‚Äêwise fine-tuning of nnU-Net on BCBM")
parser.add_argument("--batch-size",   type=int,   default=16)
parser.add_argument("--lr",           type=float, default=1e-4)
parser.add_argument("--epochs",       type=int,   default=50)
parser.add_argument("--img-size",     type=int, nargs=2, default=[320,384],
                    help="H W for 2D slices")
parser.add_argument("--raw-data-dir", type=str,   required=True,
                    help="nnUNet_raw/Dataset002_BCBM root")
parser.add_argument("--prep-data-dir",type=str,   required=True,
                    help="nnUNet_preprocessed/Dataset002_BCBM root")
parser.add_argument("--pretrained",   type=str,   required=True,
                    help="Path to BraTS pretrained checkpoint_final.pth")
parser.add_argument("--save-path",    type=str,   required=True,
                    help="Where to dump finetuned weights")
==================== recovery/vscode_history/tfk4.py ====================
#!/usr/bin/env python3
import os
import pandas as pd
import torch
from torch import nn
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from nnunetv2.training.dataloading.data_loader import nnUNetDataLoader
from batchgenerators.dataloading.nondet_multi_threaded_augmenter import NonDetMultiThreadedAugmenter

class PropertiesTransform:
    def __init__(self, label_map):
        """
        label_map: dict mapping case_id -> 0/1
        """
        self.label_map = label_map

    def __call__(self, **sample):
        """
        sample comes in with keys 'data', 'target', 'keys', ...
        we inject a 'properties' entry which is a list of dicts for each case
        """
        batch_case_ids = sample.get("keys", [])
        props = []
        for cid in batch_case_ids:
            # our metadata uses IDs like "BCBM_0001"
==================== recovery/vscode_history/tnRK.py ====================
#!/usr/bin/env python
# scripts/train_her2_unet_transfer.py

import argparse
import os
import glob

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import nibabel as nib

# -----------------------------------------------------------------------------
# 3D U-Net + classification head
# -----------------------------------------------------------------------------
class UNet3D(nn.Module):
    def __init__(self, in_ch, base_feats=32):
        super().__init__()
        # encoder
        self.enc1 = self._block(in_ch, base_feats)
        self.enc2 = self._block(base_feats, base_feats*2)
        self.enc3 = self._block(base_feats*2, base_feats*4)
        self.enc4 = self._block(base_feats*4, base_feats*8)
==================== recovery/vscode_history/tuAs.py ====================
import os
import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from batchgenerators.utilities.file_and_folder_operations import load_json # Required to load JSON files

print("Starting TorchScript tracing...")

# === Paths ===
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
save_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/nnunet_model_traced.pt"

# Define paths to nnUNet plans and dataset JSON files
# These are crucial for nnUNetTrainer initialization
plans_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans.json"
dataset_json_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/dataset.json"


# === Make sure the output directory exists
os.makedirs(os.path.dirname(save_path), exist_ok=True)

# === Load JSON contents needed for trainer initialization ===
try:
    dataset_json_content = load_json(dataset_json_path)
    # plans_json_content = load_json(plans_path) # Not strictly needed if `plans_path` is passed directly
except FileNotFoundError as e:
==================== recovery/vscode_history/u22s.py ====================
import os
import numpy as np
import nibabel as nib
from medpy.metric.binary import dc
import pandas as pd

print("Script started")

pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/gt_flat"
results = []

# Gather predictions
pred_files = sorted([f for f in os.listdir(pred_dir) if f.endswith(".nii.gz")])
print(f"Found {len(pred_files)} predicted files in: {pred_dir}")

for pred_file in pred_files:
    case_id = pred_file.replace(".nii.gz", "")
    pred_path = os.path.join(pred_dir, pred_file)
    gt_path = os.path.join(gt_dir, pred_file)  # same name assumed

    if not os.path.isfile(gt_path):
        print(f"  üö´ Missing ground truth: {gt_path}")
        continue

==================== recovery/vscode_history/uU45.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random
import warnings

import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Optional augment imports
try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
==================== recovery/vscode_history/vB9X.py ====================
#!/usr/bin/env python
import os

# 1) point nnU-Net at your data/checkpoints BEFORE importing anything from nnunetv2
os.environ["nnUNet_raw_data_base"]   = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
os.environ["nnUNet_preprocessed"]    = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
os.environ["nnUNet_results"]         = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results"

import torch
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer

# 2) paths to your BraTS plans, dataset.json, and checkpoint
PLANS_DIR    = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans_3d_fullres"
DATASET_JSON = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/dataset.json"
CHECKPOINT   = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_final.pth"
FOLD         = 3
DEVICE       = "cuda"  # or "cpu"

# 3) instantiate trainer (positional args: plans, configuration, fold, dataset_json, device)
trainer = nnUNetTrainer(
    PLANS_DIR,
    "3d_fullres",
    FOLD,
    DATASET_JSON,
    DEVICE
==================== recovery/vscode_history/wD9d.py ====================
import os
import numpy as np
import pandas as pd
import nibabel as nib
from glob import glob
from tqdm import tqdm


gt_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/labelsTr"
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats"
output_csv_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"


pred_files = glob(os.path.join(pred_dir, "fold_*", "*.nii.gz"))
if not pred_files:
    pred_files = glob(os.path.join(pred_dir, "*.nii.gz"))
print(f"Found {len(pred_files)} prediction files.")

def dice_score(gt, pred):
    """
    Calculates the Dice similarity coefficient between two binary masks.
    Returns np.nan if both masks are empty.
    """
    # Ensure inputs are boolean
    gt = gt > 0
==================== recovery/vscode_history/wOhk.py ====================
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import pandas as pd

# BCBM results
bcbm_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/bcbm_results/per_patient_analysis/dice_with_hausdorff.csv"
bcbm_df = pd.read_csv(bcbm_path)
bcbm_dice = bcbm_df['Dice'].mean()
bcbm_hd95 = bcbm_df['Hausdorff'].mean()

# BraTS results
brats_dice_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_per_class.csv"
brats_hd_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/hausdorff_sample.csv"
brats_dice_df = pd.read_csv(brats_dice_path)
brats_hd_df = pd.read_csv(brats_hd_path)

brats_dice = brats_dice_df[['ET', 'TC', 'WT']].mean().mean()
brats_hd95 = brats_hd_df[['ET', 'TC', 'WT']].mean().mean()

# Adversarial BraTS results
adv_path = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks_multi_run_v2/adversarial_attack_summary.csv"
adv_df = pd.read_csv(adv_path)
adv_dice = adv_df[['ET_dice', 'TC_dice', 'WT_dice']].mean().mean()
adv_hd95 = adv_df[['ET_hd95', 'TC_hd95', 'WT_hd95']].mean().mean()

==================== recovery/vscode_history/wV9k.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO

print("Starting FGSM attack...")
print("---")

# --- Settings ---
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# --- Paths ---
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

==================== recovery/vscode_history/wXPo.py ====================
#!/usr/bin/env python
# scripts/trainher2_unet2d_transfer_tweaked.py

import argparse
import os
import random

import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# ---------------------------------------------------------------------
#  2D U-Net + classifier
# ---------------------------------------------------------------------
class UNet2D(nn.Module):
    def __init__(self, in_ch, base_feats=16, dropout=0.0):
==================== recovery/vscode_history/x0on.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:  # Handle cases with no tumor
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with its segmentation overlay."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.5, interpolation='none')
    ax.axis('off')

# --- 1. SETTINGS ---
# Define the base directories for your data
gt_dir      = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
==================== recovery/vscode_history/xN76.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json

print("Starting FGSM attack...")

# === Settings ===
patient_id = "BraTS2021_00005"
slice_idx = 75 # Still useful for visualization of a specific slice later
# modality_idx is no longer needed for loading, as we load all 4
epsilon = 0.03 # Perturbation strength for FGSM

# === Paths ===
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

# === Set the correct path to your nnUNetPlans.json file ===
plans_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans.json"
==================== recovery/vscode_history/xpQE.py ====================
import pandas as pd
import matplotlib.pyplot as plt
import os

file_path = '/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/dice_summary.csv'

output_dir = '/sharedscratch/an252/cancerdetectiondataset/nnUNet_predictions/val_brats/figures'
output_filename = 'dice_summary_visualization.png'
output_path = os.path.join(output_dir, output_filename)

try:
    df = pd.read_csv(file_path)

    # data extraction and renaming 
    # The last two rows of the nnU-Net summary contain the mean and std dev
    # Extract the second to last row for mean scores
    mean_scores = df.iloc[-2]
    # Extract the last row for standard deviation
    std_devs = df.iloc[-1]
    # Dice_fg -> Foreground
    # Dice_1 -> Enhancing Tumour (ET)
    # Dice_2 -> Tumor Core (TC)
    # Dice_3 -> Whole Tumor (WT)
    rename_map = {
        "Dice_fg": "Foreground",
==================== recovery/vscode_history/xr5J.py ====================
#!/usr/bin/env python
# scripts/trainher2unettransfer.py

import argparse
import os
import random
import time
import numpy as np
import pandas as pd
import nibabel as nib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

try:
    import torchvision.transforms.functional as TF
    from torchvision.transforms import InterpolationMode
except ImportError:
    TF = None
    InterpolationMode = None
==================== recovery/vscode_history/yC0Y.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats # Import for statistical tests
from tqdm import tqdm # Import for progress bars

print("Starting Adversarial Attack & Defense Script...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
==================== recovery/vscode_history/yHdE.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats
from tqdm import tqdm
import random

print("Starting Adversarial Attack & Defense Script for BCBM Dataset...", flush=True)
print("---", flush=True)


PERFORM_VISUALIZATION = False # disabled to prevent memory issues
PERFORM_ADVERSARIAL_TRAINING = True
PERFORM_BLACK_BOX_ATTACKS = True


==================== recovery/vscode_history/yNIH.py ====================
import os
import nibabel as nib
import numpy as np
import pandas as pd
from medpy.metric import binary
from tqdm import tqdm

print("Script started")

# ====== PATH CONFIGURATION ======
pred_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset002_BCBM/nnUNetTrainer_FrozenEncoderBCBM__nnUNetPlans__3d_fullres/fold_3/validation"
gt_dir = "/sharedscratch/an252/cancerdetectiondataset/gt_flat"
output_csv = "bcbm_fold3_val_dice.csv"

# ====== SCAN FOR FILES ======
pred_files = sorted(f for f in os.listdir(pred_dir) if f.endswith(".nii.gz"))
print(f"Found {len(pred_files)} predicted files in: {pred_dir}")

results = []

# ====== LOOP OVER PREDICTIONS ======
for pred_file in tqdm(pred_files):
    pred_path = os.path.join(pred_dir, pred_file)
    gt_filename = pred_file.replace(".nii.gz", "_0000.nii.gz")
    gt_path = os.path.join(gt_dir, gt_filename)
==================== recovery/vscode_history/yOuK.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats # Import for statistical tests
from tqdm import tqdm # Import for progress bars

print("Starting Adversarial Attack & Defense Script...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnUNet Environment Variables
# These variables MUST be set in your shell environment *before* running this script,
# or in your SLURM batch script *before* the 'python' command.
# Example:
# export nnUNet_raw="/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw"
# export nnUNet_preprocessed="/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed"
==================== recovery/vscode_history/ysOs.py ====================
import os
import numpy as np
import nibabel as nib
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def find_best_slice(mask_data):
    """Finds the slice index with the largest tumor area for each view."""
    if np.sum(mask_data) == 0:
        shape = mask_data.shape
        return shape[2] // 2, shape[1] // 2, shape[0] // 2
    axial_slice = np.sum(mask_data, axis=(0, 1)).argmax()
    sagittal_slice = np.sum(mask_data, axis=(0, 2)).argmax()
    coronal_slice = np.sum(mask_data, axis=(1, 2)).argmax()
    return axial_slice, sagittal_slice, coronal_slice

def plot_slice_with_border(ax, mri_slice, seg_slice, colormap, norm):
    """Plots a single slice with overlay and a visible border."""
    ax.imshow(np.rot90(mri_slice), cmap='gray')
    ax.imshow(np.rot90(seg_slice), cmap=colormap, norm=norm, alpha=0.5, interpolation='none')
    ax.set_xticks([])
    ax.set_yticks([])

gt_tr_dir       = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
==================== recovery/vscode_history/ytkk.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json
from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO
import pandas as pd
import seaborn as sns
from scipy import stats
from tqdm import tqdm
import random

print("Starting Adversarial Attack & Defense Script for BCBM Dataset...", flush=True)
print("---", flush=True)

# ===========================================================================
# IMPORTANT: nnU-Net Environment Variables
# These variables MUST be set in your shell environment *before* running this script.
# ===========================================================================

# --- Configuration ---
==================== recovery/vscode_history/zLhL.py ====================
import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from scipy.ndimage import binary_erosion
from scipy.spatial.distance import directed_hausdorff
from matplotlib.lines import Line2D

# === Patients with fold index ===
patients = [
    ("BraTS2021_00003", 3),
    ("BraTS2021_00014", 3),
    ("BraTS2021_00018", 3),
]

# === Paths ===
base_pred = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres"
gt_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations"
t1ce_path_base = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesVal"

outdir = os.path.join(base_pred, "fold_3/hausdorff_visuals_w_mri")
os.makedirs(outdir, exist_ok=True)

class_labels = {1: "NCR", 2: "ED", 3: "ET"}

==================== recovery/vscode_history/zoGe.py ====================
import os
import torch
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from torch.nn.functional import cross_entropy
from batchgenerators.utilities.file_and_folder_operations import load_json

print("Starting FGSM attack...")

# === Settings ===
patient_id = "BraTS2021_00005"
slice_idx = 75
epsilon = 0.03

# === Paths ===
image_dir = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/imagesTr"
gt_path = f"/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/gt_segmentations/{patient_id}.nii.gz"
checkpoint_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_results/Dataset001_BraTS/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_3/checkpoint_best.pth"
out_dir = "/sharedscratch/an252/cancerdetectiondataset/brats_attacks/fgsm_attack"

# === Set the correct paths ===
plans_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_preprocessed/Dataset001_BraTS/nnUNetPlans.json"
dataset_json_path = "/sharedscratch/an252/cancerdetectiondataset/nnUNet_raw/Dataset001_BraTS/dataset.json"
